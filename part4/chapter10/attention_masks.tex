% Attention Masks

\section{Attention Masks}

Attention masks are fundamental to controlling how special tokens interact with other tokens in the sequence. Proper mask design ensures that special tokens fulfill their intended roles while maintaining computational efficiency and semantic coherence. This section covers advanced masking strategies that go beyond simple padding masks.

\subsection{Types of Attention Masks for Special Tokens}

Different special tokens require different attention patterns to function effectively. Understanding these patterns is crucial for implementation.

\begin{lstlisting}[language=Python, caption=Comprehensive attention mask generator for special tokens]
import torch
import torch.nn as nn
import numpy as np

class SpecialTokenMaskGenerator:
    def __init__(self, tokenizer, max_length=512):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.special_token_map = self._build_special_token_map()
        
    def _build_special_token_map(self):
        """Build mapping of special token types to their IDs."""
        token_map = {}
        
        # Standard special tokens
        for attr in ['cls_token_id', 'sep_token_id', 'pad_token_id', 
                     'mask_token_id', 'unk_token_id']:
            if hasattr(self.tokenizer, attr):
                token_id = getattr(self.tokenizer, attr)
                if token_id is not None:
                    token_map[attr.replace('_id', '')] = token_id
                    
        return token_map
        
    def create_attention_mask(self, input_ids, mask_type='bidirectional'):
        """Create sophisticated attention masks for special tokens."""
        batch_size, seq_len = input_ids.shape
        
        if mask_type == 'bidirectional':
            return self._create_bidirectional_mask(input_ids)
        elif mask_type == 'causal':
            return self._create_causal_mask(input_ids)
        elif mask_type == 'prefix_lm':
            return self._create_prefix_lm_mask(input_ids)
        elif mask_type == 'custom_special':
            return self._create_custom_special_mask(input_ids)
        else:
            raise ValueError(f"Unknown mask type: {mask_type}")
            
    def _create_bidirectional_mask(self, input_ids):
        """Standard bidirectional attention with padding mask."""
        # Basic padding mask
        padding_mask = (input_ids != self.special_token_map.get('pad_token', -1))
        
        # Expand to attention dimensions
        attention_mask = padding_mask.unsqueeze(1).unsqueeze(2)
        attention_mask = attention_mask.expand(-1, 1, input_ids.size(1), -1)
        
        return attention_mask.float()
        
    def _create_causal_mask(self, input_ids):
        """Causal mask with special token considerations."""
        batch_size, seq_len = input_ids.shape
        
        # Create basic causal mask
        causal_mask = torch.tril(torch.ones(seq_len, seq_len))
        
        # Special tokens can attend to all previous positions
        cls_token_id = self.special_token_map.get('cls_token')
        if cls_token_id is not None:
            cls_positions = (input_ids == cls_token_id)
            for batch_idx in range(batch_size):
                cls_pos = cls_positions[batch_idx].nonzero(as_tuple=True)[0]
                if len(cls_pos) > 0:
                    # CLS can attend to entire sequence
                    causal_mask[cls_pos[0], :] = 1
                    
        # Apply padding mask
        padding_mask = (input_ids != self.special_token_map.get('pad_token', -1))
        combined_mask = causal_mask.unsqueeze(0) * padding_mask.unsqueeze(1)
        
        return combined_mask.unsqueeze(1).float()
        
    def _create_prefix_lm_mask(self, input_ids):
        """Prefix LM mask where prefix tokens attend bidirectionally."""
        batch_size, seq_len = input_ids.shape
        
        # Find separator token positions
        sep_token_id = self.special_token_map.get('sep_token')
        
        masks = []
        for batch_idx in range(batch_size):
            mask = torch.zeros(seq_len, seq_len)
            
            if sep_token_id is not None:
                sep_positions = (input_ids[batch_idx] == sep_token_id).nonzero(as_tuple=True)[0]
                
                if len(sep_positions) > 0:
                    # Bidirectional attention for prefix (up to first SEP)
                    prefix_end = sep_positions[0].item()
                    mask[:prefix_end+1, :prefix_end+1] = 1
                    
                    # Causal attention for suffix (after SEP)
                    if prefix_end + 1 < seq_len:
                        causal_suffix = torch.tril(torch.ones(seq_len - prefix_end - 1, 
                                                              seq_len - prefix_end - 1))
                        mask[prefix_end+1:, prefix_end+1:] = causal_suffix
                        
                        # Suffix can attend to prefix
                        mask[prefix_end+1:, :prefix_end+1] = 1
                else:
                    # No separator found, use bidirectional
                    mask = torch.ones(seq_len, seq_len)
            else:
                # No separator token defined, use bidirectional
                mask = torch.ones(seq_len, seq_len)
                
            # Apply padding mask
            valid_positions = (input_ids[batch_idx] != self.special_token_map.get('pad_token', -1))
            mask = mask * valid_positions.unsqueeze(0) * valid_positions.unsqueeze(1)
            
            masks.append(mask)
            
        return torch.stack(masks).unsqueeze(1).float()
\end{lstlisting}

\subsection{Advanced Masking Patterns}

Complex applications require sophisticated masking patterns that account for special token semantics and interaction requirements.

\begin{lstlisting}[language=Python, caption=Advanced attention masking patterns]
class AdvancedMaskingPatterns:
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
        
    def create_hierarchical_mask(self, input_ids, segment_ids=None):
        """Create hierarchical attention masks for structured inputs."""
        batch_size, seq_len = input_ids.shape
        
        # Base attention mask
        attention_mask = torch.ones(batch_size, seq_len, seq_len)
        
        if segment_ids is not None:
            # Within-segment attention
            for batch_idx in range(batch_size):
                for i in range(seq_len):
                    for j in range(seq_len):
                        # Allow attention within same segment
                        if segment_ids[batch_idx, i] == segment_ids[batch_idx, j]:
                            attention_mask[batch_idx, i, j] = 1
                        else:
                            attention_mask[batch_idx, i, j] = 0
                            
        # Special token override rules
        cls_token_id = getattr(self.tokenizer, 'cls_token_id', None)
        sep_token_id = getattr(self.tokenizer, 'sep_token_id', None)
        
        for batch_idx in range(batch_size):
            # CLS token can attend to everything
            if cls_token_id is not None:
                cls_positions = (input_ids[batch_idx] == cls_token_id).nonzero(as_tuple=True)[0]
                for pos in cls_positions:
                    attention_mask[batch_idx, pos, :] = 1
                    attention_mask[batch_idx, :, pos] = 1
                    
            # SEP tokens have limited attention
            if sep_token_id is not None:
                sep_positions = (input_ids[batch_idx] == sep_token_id).nonzero(as_tuple=True)[0]
                for pos in sep_positions:
                    # SEP only attends to segment boundaries
                    attention_mask[batch_idx, pos, :] = 0
                    attention_mask[batch_idx, pos, sep_positions] = 1
                    if cls_token_id is not None:
                        cls_positions = (input_ids[batch_idx] == cls_token_id).nonzero(as_tuple=True)[0]
                        attention_mask[batch_idx, pos, cls_positions] = 1
                        
        return attention_mask.unsqueeze(1).float()
        
    def create_sliding_window_mask(self, input_ids, window_size=128, special_token_global=True):
        """Create sliding window attention with global special tokens."""
        batch_size, seq_len = input_ids.shape
        
        # Initialize with zeros
        attention_mask = torch.zeros(batch_size, seq_len, seq_len)
        
        # Apply sliding window
        for i in range(seq_len):
            start = max(0, i - window_size // 2)
            end = min(seq_len, i + window_size // 2 + 1)
            attention_mask[:, i, start:end] = 1
            
        if special_token_global:
            # Special tokens have global attention
            special_tokens = [
                getattr(self.tokenizer, 'cls_token_id', None),
                getattr(self.tokenizer, 'sep_token_id', None),
            ]
            
            for batch_idx in range(batch_size):
                for token_id in special_tokens:
                    if token_id is not None:
                        special_positions = (input_ids[batch_idx] == token_id).nonzero(as_tuple=True)[0]
                        for pos in special_positions:
                            attention_mask[batch_idx, pos, :] = 1
                            attention_mask[batch_idx, :, pos] = 1
                            
        # Apply padding mask
        pad_token_id = getattr(self.tokenizer, 'pad_token_id', None)
        if pad_token_id is not None:
            padding_mask = (input_ids != pad_token_id)
            attention_mask = attention_mask * padding_mask.unsqueeze(1) * padding_mask.unsqueeze(2)
            
        return attention_mask.unsqueeze(1).float()
        
    def create_sparse_attention_mask(self, input_ids, sparsity_pattern='block_sparse'):
        """Create sparse attention patterns for efficiency."""
        batch_size, seq_len = input_ids.shape
        
        if sparsity_pattern == 'block_sparse':
            mask = self._create_block_sparse_mask(seq_len, block_size=64)
        elif sparsity_pattern == 'strided':
            mask = self._create_strided_mask(seq_len, stride=4)
        elif sparsity_pattern == 'random':
            mask = self._create_random_sparse_mask(seq_len, density=0.1)
        else:
            raise ValueError(f"Unknown sparsity pattern: {sparsity_pattern}")
            
        # Ensure special tokens have full attention
        cls_token_id = getattr(self.tokenizer, 'cls_token_id', None)
        
        for batch_idx in range(batch_size):
            if cls_token_id is not None:
                cls_positions = (input_ids[batch_idx] == cls_token_id).nonzero(as_tuple=True)[0]
                for pos in cls_positions:
                    mask[pos, :] = 1
                    mask[:, pos] = 1
                    
        return mask.unsqueeze(0).unsqueeze(0).expand(batch_size, 1, -1, -1).float()
        
    def _create_block_sparse_mask(self, seq_len, block_size=64):
        """Create block-sparse attention mask."""
        mask = torch.zeros(seq_len, seq_len)
        
        for i in range(0, seq_len, block_size):
            for j in range(0, seq_len, block_size):
                end_i = min(i + block_size, seq_len)
                end_j = min(j + block_size, seq_len)
                
                # Diagonal blocks
                if abs(i - j) <= block_size:
                    mask[i:end_i, j:end_j] = 1
                    
        return mask
        
    def _create_strided_mask(self, seq_len, stride=4):
        """Create strided attention mask."""
        mask = torch.zeros(seq_len, seq_len)
        
        for i in range(seq_len):
            # Local attention
            start = max(0, i - stride)
            end = min(seq_len, i + stride + 1)
            mask[i, start:end] = 1
            
            # Strided attention
            for j in range(0, seq_len, stride):
                mask[i, j] = 1
                
        return mask
\end{lstlisting}

\subsection{Dynamic Attention Masking}

Dynamic masking allows attention patterns to adapt based on input content and model state.

\begin{lstlisting}[language=Python, caption=Dynamic attention masking based on content]
class DynamicAttentionMasking(nn.Module):
    def __init__(self, hidden_size=768, num_heads=12):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        
        # Learned masking parameters
        self.mask_predictor = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Linear(hidden_size // 2, 1),
            nn.Sigmoid()
        )
        
        # Special token attention controllers
        self.special_token_controllers = nn.ModuleDict({
            'cls_controller': nn.Linear(hidden_size, num_heads),
            'sep_controller': nn.Linear(hidden_size, num_heads),
            'mask_controller': nn.Linear(hidden_size, num_heads)
        })
        
    def forward(self, hidden_states, input_ids, base_attention_mask):
        """Generate dynamic attention masks."""
        batch_size, seq_len, _ = hidden_states.shape
        
        # Predict attention weights for each position
        attention_weights = self.mask_predictor(hidden_states).squeeze(-1)
        
        # Create position-wise mask
        position_mask = attention_weights.unsqueeze(1) * attention_weights.unsqueeze(2)
        
        # Apply special token rules
        special_token_mask = self._apply_special_token_rules(
            hidden_states, input_ids, position_mask
        )
        
        # Combine with base mask
        final_mask = base_attention_mask * special_token_mask
        
        return final_mask
        
    def _apply_special_token_rules(self, hidden_states, input_ids, position_mask):
        """Apply learned rules for special token attention."""
        batch_size, seq_len, _ = hidden_states.shape
        special_mask = position_mask.clone()
        
        # Process each special token type
        special_tokens = {
            'cls_token_id': 'cls_controller',
            'sep_token_id': 'sep_controller', 
            'mask_token_id': 'mask_controller'
        }
        
        for token_attr, controller_name in special_tokens.items():
            token_id = getattr(self.tokenizer, token_attr, None)
            if token_id is not None and controller_name in self.special_token_controllers:
                controller = self.special_token_controllers[controller_name]
                
                # Find positions of this special token
                token_positions = (input_ids == token_id)
                
                if token_positions.any():
                    # Get hidden states for these positions
                    token_hidden = hidden_states[token_positions]
                    
                    # Predict attention modulation
                    attention_modulation = controller(token_hidden)  # [num_tokens, num_heads]
                    
                    # Apply modulation to attention mask
                    for batch_idx in range(batch_size):
                        batch_positions = token_positions[batch_idx].nonzero(as_tuple=True)[0]
                        
                        for i, pos in enumerate(batch_positions):
                            # Modulate attention from this position
                            modulation = attention_modulation[i].mean()  # Average over heads
                            special_mask[batch_idx, pos, :] *= modulation
                            
        return special_mask

class ConditionalMasking:
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
        
    def create_task_conditional_mask(self, input_ids, task_type='classification'):
        """Create attention masks based on task requirements."""
        batch_size, seq_len = input_ids.shape
        
        if task_type == 'classification':
            return self._classification_mask(input_ids)
        elif task_type == 'generation':
            return self._generation_mask(input_ids)
        elif task_type == 'question_answering':
            return self._qa_mask(input_ids)
        elif task_type == 'summarization':
            return self._summarization_mask(input_ids)
        else:
            # Default bidirectional mask
            return self._default_mask(input_ids)
            
    def _classification_mask(self, input_ids):
        """Attention mask optimized for classification tasks."""
        batch_size, seq_len = input_ids.shape
        
        # Full bidirectional attention
        attention_mask = torch.ones(batch_size, seq_len, seq_len)
        
        # CLS token gets enhanced attention to all positions
        cls_token_id = getattr(self.tokenizer, 'cls_token_id', None)
        if cls_token_id is not None:
            cls_positions = (input_ids == cls_token_id)
            
            # Boost attention from CLS to all other tokens
            for batch_idx in range(batch_size):
                cls_pos = cls_positions[batch_idx].nonzero(as_tuple=True)[0]
                if len(cls_pos) > 0:
                    attention_mask[batch_idx, cls_pos[0], :] = 1.5  # Enhanced attention
                    
        # Apply padding mask
        return self._apply_padding_mask(attention_mask, input_ids)
        
    def _generation_mask(self, input_ids):
        """Causal mask for generation tasks."""
        seq_len = input_ids.size(1)
        
        # Causal mask
        causal_mask = torch.tril(torch.ones(seq_len, seq_len))
        
        # Special tokens can attend to full context
        special_tokens = [
            getattr(self.tokenizer, 'cls_token_id', None),
            getattr(self.tokenizer, 'sep_token_id', None)
        ]
        
        for batch_idx in range(input_ids.size(0)):
            for token_id in special_tokens:
                if token_id is not None:
                    positions = (input_ids[batch_idx] == token_id).nonzero(as_tuple=True)[0]
                    for pos in positions:
                        causal_mask[pos, :pos+1] = 1  # Can attend to all previous
                        
        mask = causal_mask.unsqueeze(0).expand(input_ids.size(0), -1, -1)
        return self._apply_padding_mask(mask, input_ids)
        
    def _apply_padding_mask(self, attention_mask, input_ids):
        """Apply padding mask to attention matrix."""
        pad_token_id = getattr(self.tokenizer, 'pad_token_id', None)
        if pad_token_id is not None:
            padding_mask = (input_ids != pad_token_id)
            attention_mask = attention_mask * padding_mask.unsqueeze(1) * padding_mask.unsqueeze(2)
            
        return attention_mask.unsqueeze(1).float()
\end{lstlisting}

\subsection{Attention Mask Optimization}

Optimizing attention masks can significantly improve both performance and computational efficiency.

\begin{lstlisting}[language=Python, caption=Attention mask optimization techniques]
class AttentionMaskOptimizer:
    def __init__(self):
        self.mask_cache = {}
        self.optimization_stats = {}
        
    def optimize_mask_computation(self, input_ids, mask_type='bidirectional'):
        """Optimize mask computation with caching and vectorization."""
        
        # Create cache key
        cache_key = self._create_cache_key(input_ids, mask_type)
        
        if cache_key in self.mask_cache:
            return self.mask_cache[cache_key]
            
        # Vectorized mask computation
        if mask_type == 'bidirectional':
            mask = self._vectorized_bidirectional_mask(input_ids)
        elif mask_type == 'causal':
            mask = self._vectorized_causal_mask(input_ids)
        else:
            mask = self._fallback_mask_computation(input_ids, mask_type)
            
        # Cache result
        if len(self.mask_cache) < 1000:  # Prevent unlimited growth
            self.mask_cache[cache_key] = mask
            
        return mask
        
    def _vectorized_bidirectional_mask(self, input_ids):
        """Highly optimized bidirectional mask computation."""
        batch_size, seq_len = input_ids.shape
        
        # Vectorized padding mask
        pad_token_id = getattr(self.tokenizer, 'pad_token_id', -1)
        valid_mask = (input_ids != pad_token_id).float()
        
        # Outer product for attention mask
        attention_mask = torch.bmm(
            valid_mask.unsqueeze(2),
            valid_mask.unsqueeze(1)
        )
        
        return attention_mask.unsqueeze(1)
        
    def _vectorized_causal_mask(self, input_ids):
        """Optimized causal mask with special token handling."""
        batch_size, seq_len = input_ids.shape
        
        # Base causal mask
        causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=input_ids.device))
        
        # Apply to batch
        batch_mask = causal_mask.unsqueeze(0).expand(batch_size, -1, -1)
        
        # Padding mask
        pad_token_id = getattr(self.tokenizer, 'pad_token_id', -1)
        valid_mask = (input_ids != pad_token_id).float()
        
        # Combine masks
        final_mask = batch_mask * valid_mask.unsqueeze(1) * valid_mask.unsqueeze(2)
        
        return final_mask.unsqueeze(1)
        
    def compress_sparse_mask(self, attention_mask, sparsity_threshold=0.1):
        """Compress sparse attention masks for memory efficiency."""
        
        # Identify sparse regions
        density = attention_mask.mean(dim=-1, keepdim=True)
        sparse_regions = density < sparsity_threshold
        
        # Create compressed representation
        compressed_mask = attention_mask.clone()
        compressed_mask[sparse_regions.expand_as(attention_mask)] = 0
        
        # Store compression statistics
        original_nonzeros = attention_mask.nonzero().size(0)
        compressed_nonzeros = compressed_mask.nonzero().size(0)
        compression_ratio = compressed_nonzeros / original_nonzeros
        
        self.optimization_stats['compression_ratio'] = compression_ratio
        
        return compressed_mask
        
    def adaptive_masking_threshold(self, attention_weights, percentile=90):
        """Adaptively threshold attention weights to create sparse masks."""
        
        # Compute threshold per head and layer
        threshold = torch.quantile(attention_weights, percentile / 100.0, dim=-1, keepdim=True)
        
        # Create adaptive mask
        adaptive_mask = (attention_weights >= threshold).float()
        
        # Ensure minimum connectivity
        min_connections = max(1, attention_weights.size(-1) // 10)
        top_k_mask = torch.zeros_like(attention_weights)
        
        # Keep top-k connections for each query
        _, top_indices = torch.topk(attention_weights, min_connections, dim=-1)
        top_k_mask.scatter_(-1, top_indices, 1)
        
        # Combine adaptive and top-k masks
        final_mask = torch.maximum(adaptive_mask, top_k_mask)
        
        return final_mask
        
    def _create_cache_key(self, input_ids, mask_type):
        """Create cache key for mask caching."""
        # Simple hash based on sequence length and special token positions
        seq_len = input_ids.size(1)
        
        # Find special token positions
        special_positions = []
        special_tokens = [0, 1, 2, 3, 4]  # Common special token IDs
        
        for token_id in special_tokens:
            positions = (input_ids == token_id).nonzero(as_tuple=True)
            if len(positions[0]) > 0:
                special_positions.extend(positions[1].tolist())
                
        # Create hash
        cache_key = f"{mask_type}_{seq_len}_{hash(tuple(sorted(special_positions)))}"
        return cache_key
\end{lstlisting}

\subsection{Best Practices for Attention Mask Implementation}

When implementing attention masks for special tokens, consider these best practices:

\begin{itemize}
\item \textbf{Efficiency}: Use vectorized operations and caching for mask computation
\item \textbf{Flexibility}: Design masks that can adapt to different sequence structures
\item \textbf{Semantics}: Ensure masks align with the intended behavior of special tokens
\item \textbf{Sparsity}: Leverage sparsity patterns to reduce computational overhead
\item \textbf{Dynamic Adaptation}: Allow masks to adapt based on input content when beneficial
\item \textbf{Testing}: Thoroughly test mask patterns with different input configurations
\item \textbf{Memory Management}: Implement efficient storage for large attention matrices
\item \textbf{Gradient Flow}: Ensure masks don't impede necessary gradient flow during training
\end{itemize}