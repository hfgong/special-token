% Masked Image Modeling Section

\section{Masked Image Modeling}

Masked Image Modeling (MIM) represents a fundamental adaptation of the masked language modeling paradigm from NLP to computer vision. Unlike text, where masking individual tokens (words or subwords) creates natural prediction tasks, masking image patches requires careful consideration of spatial structure and visual semantics.
\begin{comment}
Feedback: This is a good introduction. To make the core challenge clearer, you could add: "The central challenge of MIM is that image data is dense and spatially continuous, unlike the sparse, discrete nature of text. A missing word in a sentence has a clear semantic gap, but a missing patch in an image can often be easily 'cheated' by interpolating from its immediate neighbors. Therefore, effective visual masking strategies must be designed to create a genuinely difficult and meaningful reconstruction task for the model."
\end{comment}

The \mask{} token in vision transformers serves as a learnable placeholder that encourages the model to understand spatial relationships and visual context through reconstruction objectives. This approach has proven instrumental in self-supervised pre-training of vision transformers, leading to robust visual representations.

\subsection{Fundamentals of Visual Masking}

Visual masking strategies must address the unique characteristics of image data compared to text sequences. Images contain dense, correlated information where neighboring pixels share strong dependencies, making naive random masking less effective than structured approaches.

\begin{definition}[Visual Mask Token]
A Visual Mask token is a learnable parameter that replaces selected image patches during pre-training. It serves as a reconstruction target, forcing the model to predict the original patch content based on surrounding visual context and learned spatial relationships.
\end{definition}

The mathematical formulation for masked image modeling follows this structure:

\begin{align}
\mathbf{x}_{\text{masked}} &= \text{MASK}(\mathbf{x}, \mathcal{M}) \\
\hat{\mathbf{x}}_{\mathcal{M}} &= f_{\theta}(\mathbf{x}_{\text{masked}}) \\
\mathcal{L}_{\text{MIM}} &= \frac{1}{|\mathcal{M}|} \sum_{i \in \mathcal{M}} \ell(\mathbf{x}_i, \hat{\mathbf{x}}_i)
\end{align}

where $\mathcal{M}$ represents the set of masked patch indices, $f_{\theta}$ is the vision transformer, and $\ell$ is the reconstruction loss function.

\subsection{Masking Strategies}

Different masking strategies have emerged to optimize the learning signal while maintaining computational efficiency.

\subsubsection{Random Masking}

The simplest approach randomly selects patches for masking:

\begin{lstlisting}[language=Python, caption=Random masking implementation for vision transformers]
def random_masking(x, mask_ratio=0.75):
    """
    Random masking of image patches for MAE-style pre-training.
    
    Args:
        x: [B, N, D] tensor of patch embeddings
        mask_ratio: fraction of patches to mask
    
    Returns:
        x_masked: [B, N_visible, D] visible patches
        mask: [B, N] binary mask (0 for masked, 1 for visible)
        ids_restore: [B, N] indices to restore original order
    """
    B, N, D = x.shape
    len_keep = int(N * (1 - mask_ratio))
    
    # Generate random permutation
    noise = torch.rand(B, N, device=x.device)
    ids_shuffle = torch.argsort(noise, dim=1)
    ids_restore = torch.argsort(ids_shuffle, dim=1)
    
    # Keep subset of patches
    ids_keep = ids_shuffle[:, :len_keep]
    x_masked = torch.gather(x, dim=1, 
                           index=ids_keep.unsqueeze(-1).repeat(1, 1, D))
    
    # Generate binary mask: 0 for masked, 1 for visible
    mask = torch.ones([B, N], device=x.device)
    mask[:, :len_keep] = 0
    mask = torch.gather(mask, dim=1, index=ids_restore)
    
    return x_masked, mask, ids_restore
\end{lstlisting}

\subsubsection{Block-wise Masking}

Block-wise masking creates contiguous masked regions, which better reflects natural occlusion patterns:

\begin{lstlisting}[language=Python, caption=Block-wise masking for structured visual learning]
def block_wise_masking(x, block_size=4, mask_ratio=0.75):
    """
    Block-wise masking creating contiguous masked regions.
    """
    B, N, D = x.shape
    H = W = int(math.sqrt(N))  # Assume square image
    
    # Reshape to spatial grid
    x_spatial = x.view(B, H, W, D)
    
    # Calculate number of blocks to mask
    num_blocks_h = H // block_size
    num_blocks_w = W // block_size
    total_blocks = num_blocks_h * num_blocks_w
    num_masked_blocks = int(total_blocks * mask_ratio)
    
    mask = torch.zeros(B, H, W, device=x.device)
    
    for b in range(B):
        # Randomly select blocks to mask
        block_indices = torch.randperm(total_blocks)[:num_masked_blocks]
        
        for idx in block_indices:
            block_h = idx // num_blocks_w
            block_w = idx % num_blocks_w
            
            start_h = block_h * block_size
            end_h = start_h + block_size
            start_w = block_w * block_size
            end_w = start_w + block_size
            
            mask[b, start_h:end_h, start_w:end_w] = 1
    
    # Convert back to sequence format
    mask_seq = mask.view(B, N)
    
    return apply_mask(x, mask_seq), mask_seq
\end{lstlisting}

\subsubsection{Content-Aware Masking}

Advanced masking strategies consider image content to create more challenging reconstruction tasks:

\begin{lstlisting}[language=Python, caption=Content-aware masking based on patch importance]
def content_aware_masking(x, attention_weights, mask_ratio=0.75):
    """
    Mask patches based on attention importance scores.
    
    Args:
        x: [B, N, D] patch embeddings
        attention_weights: [B, N] importance scores
        mask_ratio: fraction of patches to mask
    """
    B, N, D = x.shape
    len_keep = int(N * (1 - mask_ratio))
    
    # Sort patches by importance (ascending for harder task)
    _, ids_sorted = torch.sort(attention_weights, dim=1)
    
    # Mask most important patches (harder reconstruction)
    ids_keep = ids_sorted[:, :len_keep]
    ids_masked = ids_sorted[:, len_keep:]
    
    # Create visible subset
    x_masked = torch.gather(x, dim=1,
                           index=ids_keep.unsqueeze(-1).repeat(1, 1, D))
    
    # Generate mask
    mask = torch.zeros(B, N, device=x.device)
    mask.scatter_(1, ids_masked, 1)
    
    return x_masked, mask, ids_keep
\end{lstlisting}

\subsection{Reconstruction Targets}

The choice of reconstruction target significantly impacts learning quality. Different approaches optimize for various aspects of visual understanding.

\subsubsection{Pixel-Level Reconstruction}

Direct pixel reconstruction optimizes for low-level visual features:

\begin{align}
\mathcal{L}_{\text{pixel}} = \frac{1}{|\mathcal{M}|} \sum_{i \in \mathcal{M}} \|\mathbf{p}_i - \hat{\mathbf{p}}_i\|_2^2
\end{align}

where $\mathbf{p}_i$ and $\hat{\mathbf{p}}_i$ are original and predicted pixel values.

\subsubsection{Feature-Level Reconstruction}

Higher-level feature reconstruction encourages semantic understanding:

\begin{lstlisting}[language=Python, caption=Feature-level reconstruction using pre-trained encoders]
class FeatureReconstructionMAE(nn.Module):
    def __init__(self, encoder_dim=768, feature_extractor='dino'):
        super().__init__()
        
        self.encoder = ViTEncoder(embed_dim=encoder_dim)
        self.decoder = MAEDecoder(embed_dim=encoder_dim)
        
        # Pre-trained feature extractor (frozen)
        if feature_extractor == 'dino':
            self.feature_extractor = torch.hub.load('facebookresearch/dino:main', 
                                                   'dino_vits16')
            self.feature_extractor.eval()
            for param in self.feature_extractor.parameters():
                param.requires_grad = False
    
    def forward(self, x, mask):
        # Encode visible patches
        latent = self.encoder(x, mask)
        
        # Decode to reconstruct
        pred = self.decoder(latent, mask)
        
        # Extract target features
        with torch.no_grad():
            target_features = self.feature_extractor(x)
        
        # Compute feature reconstruction loss
        pred_features = self.feature_extractor(pred)
        loss = F.mse_loss(pred_features, target_features)
        
        return pred, loss
\end{lstlisting}

\subsubsection{Contrastive Reconstruction}

Contrastive approaches encourage learning discriminative representations:

\begin{align}
\mathcal{L}_{\text{contrast}} = -\log \frac{\exp(\text{sim}(\mathbf{z}_i, \mathbf{z}_i^+) / \tau)}{\sum_{j} \exp(\text{sim}(\mathbf{z}_i, \mathbf{z}_j) / \tau)}
\end{align}

where $\mathbf{z}_i^+$ represents positive examples and $\tau$ is the temperature parameter.

\subsection{Architectural Considerations}

Effective masked image modeling requires careful architectural design to balance reconstruction quality with computational efficiency.

\subsubsection{Asymmetric Encoder-Decoder Design}

The MAE architecture employs an asymmetric design with a heavy encoder and lightweight decoder:
\begin{comment}
Feedback: This is a critical architectural insight that deserves more explanation. Before the code, it would be great to explain the "why". For example: "The key insight of the Masked Autoencoder (MAE) is that the encoder only needs to process the small subset of *visible* patches, while the lightweight decoder is responsible for reconstructing the full image from the encoded representation plus the [MASK] tokens. Since a high masking ratio is used (e.g., 75%), this makes pre-training significantly faster and more memory-efficient than processing the full set of patches in the encoder."
\end{comment}

\begin{lstlisting}[language=Python, caption={Asymmetric MAE architecture implementation}]
# Complete implementation available at:
# https://github.com/hfgong/special-token/blob/main/code/part2/chapter04/masked_image_modeling_asymmetric_mae_architecture_im.py

# See the external file for the complete implementation
# File: code/part2/chapter04/masked_image_modeling_asymmetric_mae_architecture_im.py
# Lines: 67

class ImplementationReference:
    """Asymmetric MAE architecture implementation
    
    The complete implementation is available in the external code file.
    This placeholder reduces the book's verbosity while maintaining
    access to all implementation details.
    """
    pass
\end{lstlisting}

\subsection{Training Strategies and Optimization}

Successful masked image modeling requires careful training strategies to achieve stable and effective learning.

\subsubsection{Progressive Masking}

Progressive masking gradually increases masking difficulty during training:

\begin{lstlisting}[language=Python, caption=Progressive masking curriculum for stable training]
class ProgressiveMaskingScheduler:
    def __init__(self, initial_ratio=0.25, final_ratio=0.75, total_steps=100000):
        self.initial_ratio = initial_ratio
        self.final_ratio = final_ratio
        self.total_steps = total_steps
    
    def get_mask_ratio(self, step):
        """Get current masking ratio based on training progress."""
        if step >= self.total_steps:
            return self.final_ratio
        
        progress = step / self.total_steps
        # Cosine annealing schedule
        ratio = self.final_ratio + 0.5 * (self.initial_ratio - self.final_ratio) * \
                (1 + math.cos(math.pi * progress))
        
        return ratio

# Usage in training loop
scheduler = ProgressiveMaskingScheduler()

for step, batch in enumerate(dataloader):
    current_mask_ratio = scheduler.get_mask_ratio(step)
    x_masked, mask, ids_restore = random_masking(batch, current_mask_ratio)
    
    # Forward pass and loss computation
    pred = model(x_masked, mask, ids_restore)
    loss = compute_reconstruction_loss(pred, batch, mask)
\end{lstlisting}

\subsubsection{Multi-Scale Training}

Training on multiple resolutions improves robustness:

\begin{lstlisting}[language=Python, caption=Multi-scale masked image modeling training]
def multi_scale_mae_training(model, batch, scales=[224, 256, 288]):
    """
    Train MAE with multiple input scales for robustness.
    """
    total_loss = 0
    
    for scale in scales:
        # Resize input to current scale
        batch_scaled = F.interpolate(batch, size=(scale, scale), 
                                   mode='bicubic', align_corners=False)
        
        # Apply masking
        x_masked, mask, ids_restore = random_masking(
            model.patch_embed(batch_scaled)
        )
        
        # Forward pass
        pred = model(x_masked, mask, ids_restore)
        
        # Compute loss for masked patches only
        target = model.patchify(batch_scaled)
        loss = F.mse_loss(pred[mask], target[mask])
        
        total_loss += loss / len(scales)
    
    return total_loss
\end{lstlisting}

\subsection{Evaluation and Analysis}

Understanding the effectiveness of masked image modeling requires comprehensive evaluation across multiple dimensions.

\subsubsection{Reconstruction Quality Metrics}

Various metrics assess reconstruction fidelity:

\begin{lstlisting}[language=Python, caption=Comprehensive evaluation of MAE reconstruction quality]
def evaluate_mae_reconstruction(model, dataloader, device):
    """Comprehensive evaluation of MAE reconstruction quality."""
    model.eval()
    
    total_mse = 0
    total_psnr = 0
    total_ssim = 0
    num_samples = 0
    
    with torch.no_grad():
        for batch in dataloader:
            batch = batch.to(device)
            
            # Forward pass
            x_masked, mask, ids_restore = random_masking(
                model.patch_embed(batch)
            )
            pred = model(x_masked, mask, ids_restore)
            
            # Convert predictions back to images
            pred_images = model.unpatchify(pred)
            
            # Compute metrics
            mse = F.mse_loss(pred_images, batch)
            psnr = compute_psnr(pred_images, batch)
            ssim = compute_ssim(pred_images, batch)
            
            total_mse += mse.item()
            total_psnr += psnr.item()
            total_ssim += ssim.item()
            num_samples += 1
    
    return {
        'mse': total_mse / num_samples,
        'psnr': total_psnr / num_samples,
        'ssim': total_ssim / num_samples
    }

def compute_psnr(pred, target):
    """Compute Peak Signal-to-Noise Ratio."""
    mse = F.mse_loss(pred, target)
    psnr = 20 * torch.log10(1.0 / torch.sqrt(mse))
    return psnr

def compute_ssim(pred, target):
    """Compute Structural Similarity Index."""
    # Implementation using kornia or custom SSIM
    from kornia.losses import ssim_loss
    return 1 - ssim_loss(pred, target, window_size=11)
\end{lstlisting}

\subsection{Best Practices and Guidelines}

Based on extensive research and empirical studies, several best practices emerge for effective masked image modeling:

\begin{enumerate}
\item \textbf{High Masking Ratios}: Use aggressive masking (75\%+) for meaningful reconstruction challenges
\item \textbf{Asymmetric Architecture}: Employ lightweight decoders to focus computation on encoding
\item \textbf{Proper Initialization}: Initialize mask tokens with small random values
\item \textbf{Position Embedding Integration}: Include comprehensive position information
\item \textbf{Progressive Training}: Start with easier tasks and increase difficulty
\item \textbf{Multi-Scale Robustness}: Train on various input resolutions
\item \textbf{Careful Target Selection}: Choose reconstruction targets aligned with downstream tasks
\end{enumerate}
\begin{comment}
Feedback: This is a great summary. To make it more actionable:
1.  **High Masking Ratios**: "Don't be afraid to use very high masking ratios like 75% or even 80%. Unlike in NLP, the high spatial redundancy in images means the model needs a very challenging task to learn meaningful, non-local representations."
2.  **Asymmetric Architecture**: "For efficient pre-training, ensure your encoder *only* processes the visible patches. Passing the full sequence of visible and masked tokens through a deep encoder is computationally wasteful and misses the key optimization of MAE."
3.  **Careful Target Selection**: "If your downstream task is semantic (like classification), pre-training with a feature-level reconstruction target (like DINO features) can often lead to better fine-tuning performance than simple pixel-level reconstruction, even if the reconstructed images look less visually appealing."
\end{comment}

Masked Image Modeling has revolutionized self-supervised learning in computer vision by adapting the powerful masking paradigm from NLP. The careful design of mask tokens and reconstruction objectives enables vision transformers to learn rich visual representations without requiring labeled data, making it a cornerstone technique for modern visual understanding systems.