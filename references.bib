% Foundational Papers

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017},
  note={Introduced the transformer architecture and positional encodings}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018},
  note={Introduced [CLS], [SEP], and [MASK] tokens for bidirectional pre-training}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019},
  note={GPT-2: Demonstrated the power of autoregressive modeling with special tokens}
}

@article{lewis2020bart,
  title={BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={7871--7880},
  year={2020},
  note={BART: Bidirectional encoder with autoregressive decoder, using SOS tokens for generation initialization}
}

% Vision Transformers

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020},
  note={Vision Transformer (ViT): Adapted [CLS] token for image classification}
}

@article{he2022masked,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  journal={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={16000--16009},
  year={2022},
  note={MAE: Introduced masked patch modeling with special mask tokens}
}

@article{darcet2023vision,
  title={Vision transformers need registers},
  author={Darcet, Timoth{\'e}e and Oquab, Maxime and Mairal, Julien and Bojanowski, Piotr},
  journal={arXiv preprint arXiv:2309.16588},
  year={2023},
  note={Introduced register tokens to improve ViT performance}
}

% Multimodal Models

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR},
  note={CLIP: Cross-modal alignment with special tokens}
}

@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={23716--23736},
  year={2022},
  note={Multimodal model with interleaved image and text tokens}
}

% Prompt Tuning and Soft Tokens

@article{lester2021power,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv preprint arXiv:2104.08691},
  year={2021},
  note={Introduced learnable soft prompt tokens}
}

@article{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021},
  note={Continuous prefix tokens for efficient fine-tuning}
}

% GUI and Interface Tokens (2023-2024)

@article{yang2023setofmark,
  title={The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)},
  author={Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Azarnasab, Ehsan and Ahmed, Faisal and Liu, Zicheng and Liu, Ce and Zeng, Michael and Wang, Lijuan},
  journal={arXiv preprint arXiv:2309.17421},
  year={2023},
  note={Introduced Set-of-Mark (SoM) for visual grounding and GUI interaction}
}

@article{wu2023screenai,
  title={ScreenAI: A Vision-Language Model for UI and Infographics Understanding},
  author={Wu, Gilles Baechler and Srinivasan, Srinivas and Strubell, Emma and Rohrbach, Marcus and Kiela, Douwe},
  journal={arXiv preprint arXiv:2402.04615},
  year={2024},
  note={Comprehensive screen understanding with specialized interface tokens}
}

@inproceedings{shi2017world,
  title={World of bits: An open-domain platform for web-based agents},
  author={Shi, Tianlin and Karpathy, Andrej and Fan, Linxi and Hernandez, Jonathan and Liang, Percy},
  booktitle={International Conference on Machine Learning},
  pages={3135--3144},
  year={2017},
  organization={PMLR},
  note={Early work on web automation with coordinate-based tokens}
}

@article{hong2023cogagent,
  title={CogAgent: A Visual Language Model for GUI Agents},
  author={Hong, Wenyi and Ding, Weihan and Wang, Wenmeng and Liu, Xiao and Tang, Jie},
  journal={arXiv preprint arXiv:2312.08914},
  year={2023},
  note={GUI automation agent with specialized interface understanding tokens}
}

% Reasoning and Chain-of-Thought Tokens

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022},
  note={Foundational work on step-by-step reasoning with intermediate tokens}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022},
  note={Zero-shot chain-of-thought with "Let's think step by step" prompting}
}

@article{bai2022constitutional,
  title={Constitutional ai: Harmlessness from ai feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022},
  note={Introduced constitutional AI with principle-based tokens ([HELPFUL], [HARMLESS], [HONEST])}
}

@article{yao2022react,
  title={React: Synergizing reasoning and acting in language models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  journal={arXiv preprint arXiv:2210.03629},
  year={2022},
  note={ReAct: Reasoning and acting with interleaved thought and action tokens}
}

@article{anthropic2023thinking,
  title={Constitutional AI: Harmlessness from AI Feedback},
  author={{Anthropic}},
  journal={Technical Report},
  year={2023},
  note={Internal thinking tokens and chain-of-thought reasoning mechanisms}
}

% Tool Interaction and Function Calling

@article{schick2023toolformer,
  title={Toolformer: Language models can teach themselves to use tools},
  author={Schick, Timo and Dwivedi-Yu, Jane and Dess{\`i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  journal={arXiv preprint arXiv:2302.04761},
  year={2023},
  note={Self-supervised learning of tool use with API call tokens}
}

@article{qin2023tool,
  title={Tool Learning with Foundation Models},
  author={Qin, Yujia and Liang, Shengding and Ye, Yining and Zhu, Kunlun and Yan, Lan and Lu, Yaxi and Lin, Yankai and Cong, Xin and Tang, Xiangru and Qian, Bill and others},
  journal={arXiv preprint arXiv:2304.08354},
  year={2023},
  note={Comprehensive survey and framework for tool interaction tokens}
}

@article{openai2023gpt4,
  title={GPT-4 Technical Report},
  author={OpenAI},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023},
  note={Function calling capabilities with structured tool invocation tokens}
}

@article{mialon2023augmented,
  title={Augmented language models: a survey},
  author={Mialon, Gr{\'e}goire and Dess{\`i}, Roberto and Lomeli, Maria and Nalmpantis, Christoforos and Pasunuru, Ram and Raileanu, Roberta and Roziere, Baptiste and Schick, Timo and Dwivedi-Yu, Jane and Celikyilmaz, Asli and others},
  journal={arXiv preprint arXiv:2302.07842},
  year={2023},
  note={Survey of tool-augmented language models and interaction protocols}
}

% Personalization and Identity Tokens

@article{ruiz2022dreambooth,
  title={Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation},
  author={Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir},
  journal={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={22500--22510},
  year={2023},
  note={Personalization with unique identifier tokens [V] for subject-driven generation}
}

@article{gal2022image,
  title={An image is worth one word: Personalizing text-to-image generation using textual inversion},
  author={Gal, Rinon and Alaluf, Yuval and Atzmon, Yuval and Patashnik, Or and Bermano, Amit H and Chechik, Gal and Cohen-Or, Daniel},
  journal={arXiv preprint arXiv:2208.01618},
  year={2022},
  note={Textual inversion with learnable pseudo-word tokens [S*] for personalization}
}

@article{zhang2023adding,
  title={Adding conditional control to text-to-image diffusion models},
  author={Zhang, Lvmin and Rao, Anyi and Agrawala, Maneesh},
  journal={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={3836--3847},
  year={2023},
  note={ControlNet: Conditional tokens for fine-grained control in generation}
}

% Memory and Retrieval Tokens

@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={9459--9474},
  year={2020},
  note={RAG: Retrieval-augmented generation with document boundary tokens}
}

@article{borgeaud2022improving,
  title={Improving language models by retrieving from trillions of tokens},
  author={Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others},
  journal={International conference on machine learning},
  pages={2206--2240},
  year={2022},
  organization={PMLR},
  note={RETRO: Retrieval-enhanced transformer with memory access tokens}
}

@article{wu2022memorizing,
  title={Memorizing transformers},
  author={Wu, Yuhuai and Rabe, Markus N and Hutchins, DeLesley and Szegedy, Christian},
  journal={arXiv preprint arXiv:2203.08913},
  year={2022},
  note={External memory integration with [MEM] tokens for long-context modeling}
}

@article{zhong2022training,
  title={Training language models with memory augmentation},
  author={Zhong, Zexuan and Guo, Tao and Zhang, Yiming and Wang, Chengqi and Chen, Wenqi and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2205.12674},
  year={2022},
  note={Memory-augmented training with hierarchical memory tokens}
}

% Masked Language Modeling Variants

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019},
  note={Dynamic masking strategies for [MASK] token}
}

@article{joshi2020spanbert,
  title={Spanbert: Improving pre-training by representing and predicting spans},
  author={Joshi, Mandar and Chen, Danqi and Liu, Yinhan and Weld, Daniel S and Zettlemoyer, Luke and Levy, Omer},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={64--77},
  year={2020},
  note={Span-based masking with special tokens}
}

% Memory and Retrieval Tokens

@article{wu2022memorizing,
  title={Memorizing transformers},
  author={Wu, Yuhuai and Rabe, Markus N and Hutchins, DeLesley and Szegedy, Christian},
  journal={arXiv preprint arXiv:2203.08913},
  year={2022},
  note={Memory tokens for long-range dependencies}
}

@article{borgeaud2022improving,
  title={Improving language models by retrieving from trillions of tokens},
  author={Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others},
  journal={International conference on machine learning},
  pages={2206--2240},
  year={2022},
  organization={PMLR},
  note={RETRO: Retrieval-enhanced transformers with special tokens}
}

% Code Generation

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021},
  note={Codex: Special tokens for code generation}
}

@article{nijkamp2022codegen,
  title={Codegen: An open large language model for code with multi-turn program synthesis},
  author={Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
  journal={arXiv preprint arXiv:2203.13474},
  year={2022},
  note={Multi-language code generation with language-specific tokens}
}

% Efficiency and Token Reduction

@article{rao2021dynamicvit,
  title={Dynamicvit: Efficient vision transformers with dynamic token sparsification},
  author={Rao, Yongming and Zhao, Wenliang and Liu, Benlin and Lu, Jiwen and Zhou, Jie and Hsieh, Cho-Jui},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={13937--13949},
  year={2021},
  note={Dynamic token pruning in vision transformers}
}

@article{bolya2022token,
  title={Token merging: Your vit but faster},
  author={Bolya, Daniel and Fu, Cheng-Yang and Dai, Xiaoliang and Zhang, Peizhao and Feichtenhofer, Christoph and Hoffman, Judy},
  journal={arXiv preprint arXiv:2210.09461},
  year={2022},
  note={Token merging strategies for efficiency}
}

% Chain of Thought

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022},
  note={Special tokens for reasoning chains}
}

% Tool Use

@article{schick2023toolformer,
  title={Toolformer: Language models can teach themselves to use tools},
  author={Schick, Timo and Dwivedi-Yu, Jane and Dess{\`\i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023},
  note={Special tokens for tool invocation}
}

% Position Encodings

@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Wen, Bo and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  note={Alternative position encoding strategies}
}

% Recent Advances (2023-2024)

@article{openai2024o1,
  title={Learning to Reason with LLMs},
  author={{OpenAI}},
  journal={Technical Blog},
  year={2024},
  note={o1 model with explicit reasoning tokens and chain-of-thought training}
}

@article{anthropic2024claude3,
  title={The Claude 3 Model Family: Opus, Sonnet, Haiku},
  author={{Anthropic}},
  journal={Technical Report},
  year={2024},
  note={Advanced reasoning capabilities with internal thinking processes}
}

@article{google2024gemini,
  title={Gemini: A Family of Highly Capable Multimodal Models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023},
  note={Multimodal foundation model with integrated cross-modal special tokens}
}

@article{team2024gemma,
  title={Gemma: Open Models Based on Gemini Research and Technology},
  author={{Gemma Team}},
  journal={arXiv preprint arXiv:2403.08295},
  year={2024},
  note={Open-source models with modern tokenization strategies}
}

@article{jiang2024mixtral,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Emma and Lengyel, Gianna and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024},
  note={Mixture of experts with routing tokens for specialized processing}
}

@article{dao2024mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Dao, Tri and Gu, Albert},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023},
  note={State space models with selective tokens for efficient sequence modeling}
}

@article{lin2024vila,
  title={VILA: On Pre-training for Visual Language Models},
  author={Lin, Ji and Rao, Hongxu and Lu, Jinjie and Zhou, Yangguang and Chen, Yilun and Zhang, Bo and Liu, Hanrui and Song, Han},
  journal={arXiv preprint arXiv:2312.07533},
  year={2023},
  note={Advanced vision-language models with optimized cross-modal tokens}
}

@article{qwen2024,
  title={Qwen Technical Report},
  author={{Qwen Team}},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023},
  note={Multilingual foundation models with enhanced tokenization for diverse languages}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023},
  note={Modern special token usage in large language models}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023},
  note={Efficient transformer architectures with special tokens}
}

% Tokenization and Subword Methods

@article{sennrich2016neural,
  title={Neural machine translation of rare words with subword units},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  journal={arXiv preprint arXiv:1508.07909},
  year={2016},
  note={Byte Pair Encoding (BPE) for subword tokenization}
}

@article{kudo2018subword,
  title={Subword regularization: Improving neural network translation models with multiple subword candidates},
  author={Kudo, Taku},
  journal={arXiv preprint arXiv:1804.10959},
  year={2018},
  note={SentencePiece and subword regularization techniques}
}

@inproceedings{schuster2012japanese,
  title={Japanese and korean voice search},
  author={Schuster, Mike and Nakajima, Kaisuke},
  booktitle={2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={5149--5152},
  year={2012},
  organization={IEEE},
  note={WordPiece tokenization algorithm}
}

% Attention Mechanisms and Analysis

@article{clark2019what,
  title={What does BERT look at? An analysis of BERT's attention},
  author={Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D},
  journal={arXiv preprint arXiv:1906.04341},
  year={2019},
  note={Analysis of attention patterns in BERT}
}

@article{rogers2020primer,
  title={A primer on neural network models for natural language processing},
  author={Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
  journal={Journal of Artificial Intelligence Research},
  volume={61},
  pages={65--95},
  year={2020},
  note={Comprehensive survey of NLP architectures}
}

% Performance Benchmarks

@article{wang2018glue,
  title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018},
  note={Standard benchmark for evaluating language models}
}

@article{wang2019superglue,
  title={SuperGLUE: A stickier benchmark for general-purpose language understanding systems},
  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019},
  note={Advanced benchmark suite for language understanding}
}

% Padding and Sequence Processing

@article{gehring2017convolutional,
  title={Convolutional sequence to sequence learning},
  author={Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N},
  journal={International conference on machine learning},
  pages={1243--1252},
  year={2017},
  organization={PMLR},
  note={Dynamic convolutions and padding strategies}
}

@article{press2020shortformer,
  title={Shortformer: Better language modeling using shorter inputs},
  author={Press, Ofir and Smith, Noah A and Lewis, Mike},
  journal={arXiv preprint arXiv:2012.15832},
  year={2020},
  note={Position-aware attention and sequence length optimization}
}

% ImageNet and Vision Benchmarks

@article{russakovsky2015imagenet,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International journal of computer vision},
  volume={115},
  pages={211--252},
  year={2015},
  note={Standard dataset for image classification evaluation}
}

@article{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={IEEE},
  note={ImageNet dataset creation and structure}
}

% Computational Efficiency

@article{strubell2019energy,
  title={Energy and policy considerations for deep learning in NLP},
  author={Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  journal={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={3645--3650},
  year={2019},
  note={Environmental cost analysis of transformer training}
}

@article{tay2022efficient,
  title={Efficient transformers: A survey},
  author={Tay, Yi and Dehghani, Mostafa and Rao, Jinfeng and Fedus, William and Abnar, Samira and Chung, Hyung Won and Narang, Sharan and Yogatama, Dani and Vaswani, Ashish and Metzler, Donald},
  journal={ACM Computing Surveys},
  volume={55},
  number={6},
  pages={1--28},
  year={2022},
  note={Comprehensive survey of transformer efficiency techniques}
}

% Fine-tuning and Transfer Learning

@article{howard2018universal,
  title={Universal language model fine-tuning for text classification},
  author={Howard, Jeremy and Ruder, Sebastian},
  journal={arXiv preprint arXiv:1801.06146},
  year={2018},
  note={ULMFiT: Transfer learning strategies for NLP}
}

@article{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  journal={International Conference on Machine Learning},
  pages={2790--2799},
  year={2019},
  organization={PMLR},
  note={Adapter modules and efficient fine-tuning}
}

% Masked Language Modeling and Pre-training

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019},
  note={Knowledge distillation for BERT with special token preservation}
}

@article{clark2020electra,
  title={ELECTRA: Pre-training text encoders as discriminators rather than generators},
  author={Clark, Kevin and Luong, Minh-Thang and Le, Quoc V and Manning, Christopher D},
  journal={arXiv preprint arXiv:2003.10555},
  year={2020},
  note={Alternative to MLM using replaced token detection}
}

@article{yang2019xlnet,
  title={XLNet: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan R and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019},
  note={Permutation-based language modeling without MASK tokens}
}

% Vision Transformers and Performance

@article{touvron2021training,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthieu and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  journal={International conference on machine learning},
  pages={10347--10357},
  year={2021},
  organization={PMLR},
  note={DeiT: Efficient vision transformer training and performance metrics}
}

@article{yuan2021tokens,
  title={Tokens-to-token vit: Training vision transformers from scratch on imagenet},
  author={Yuan, Li and Chen, Yunpeng and Wang, Tao and Yu, Weihao and Shi, Yujun and Jiang, Zi-Hang and Tay, Francis EH and Feng, Jiashi and Yan, Shuicheng},
  journal={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={558--567},
  year={2021},
  note={T2T-ViT: Alternative tokenization for vision transformers}
}

@article{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  journal={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10012--10022},
  year={2021},
  note={Hierarchical vision transformer with windowed attention}
}

% Multimodal and Cross-Modal

@article{tsimpoukelli2021multimodal,
  title={Multimodal few-shot learning with frozen language models},
  author={Tsimpoukelli, Maria and Menick, Jacob L and Cabi, Serkan and Eslami, SM and Vinyals, Oriol and Hill, Felix},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={200--212},
  year={2021},
  note={Frozen language models for multimodal tasks with special tokens}
}

@article{wang2022image,
  title={Image as a foreign language: BEiT pretraining for all vision and vision-language tasks},
  author={Wang, Wenhui and Bao, Hangbo and Dong, Li and Bjorck, Johan and Peng, Zhiliang and Liu, Qiang and Aggarwal, Kriti and Mohammed, Owais Khan and Singhal, Saksham and Som, Subhojit and others},
  journal={arXiv preprint arXiv:2208.10442},
  year={2022},
  note={BEiT v2: Unified vision-language pre-training}
}

@article{li2022blip,
  title={BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
  author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  journal={International Conference on Machine Learning},
  pages={12888--12900},
  year={2022},
  organization={PMLR},
  note={Unified vision-language model with special tokens}
}

% Attention Analysis and Interpretability

@article{tenney2019bert,
  title={BERT rediscovers the classical NLP pipeline},
  author={Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
  journal={arXiv preprint arXiv:1905.05950},
  year={2019},
  note={Analysis of BERT's layer-wise linguistic capabilities}
}

@article{kovaleva2019revealing,
  title={Revealing the dark secrets of BERT},
  author={Kovaleva, Olga and Romanov, Alexey and Rogers, Anna and Rumshisky, Anna},
  journal={arXiv preprint arXiv:1908.08593},
  year={2019},
  note={Analysis of BERT attention patterns and special token behavior}
}

@article{michel2019sixteen,
  title={Are sixteen heads really better than one?},
  author={Michel, Paul and Levy, Omer and Neubig, Graham},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019},
  note={Analysis of attention head importance and redundancy}
}

% Sequence-to-Sequence and Generation

@article{lewis2020bart,
  title={BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Veselin and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019},
  note={BART: Denoising autoencoder with special token masking}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  note={T5: Text-to-Text Transfer Transformer with task prefixes}
}

% Self-Supervised Learning

@article{bao2021beit,
  title={BEiT: BERT pre-training of image transformers},
  author={Bao, Hangbo and Dong, Li and Piao, Songhao and Wei, Furu},
  journal={arXiv preprint arXiv:2106.08254},
  year={2021},
  note={BERT-style pre-training for vision transformers with image tokens}
}

@article{xie2022simmim,
  title={SimMIM: A simple framework for masked image modeling},
  author={Xie, Zhenda and Zhang, Zheng and Cao, Yue and Lin, Yutong and Bao, Jianmin and Yao, Zhenkai and Dai, Qifeng and Hu, Han},
  journal={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9653--9663},
  year={2022},
  note={Simplified masked image modeling framework}
}

% Training Strategies and Optimization

@article{goyal2017accurate,
  title={Accurate, large minibatch sgd: Training imagenet in 1 hour},
  author={Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  journal={arXiv preprint arXiv:1706.02677},
  year={2017},
  note={Large batch training strategies for vision models}
}

@article{you2019large,
  title={Large batch optimization for deep learning: training BERT in 76 minutes},
  author={You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
  journal={arXiv preprint arXiv:1904.00962},
  year={2019},
  note={Large batch training for BERT with special token handling}
}

% Register Tokens and Recent Innovations

@article{dehghani2023scaling,
  title={Scaling vision transformers to 22 billion parameters},
  author={Dehghani, Mostafa and Djolonga, Josip and Mustafa, Basil and Padlewski, Piotr and Heek, Jonathan and Gilmer, Justin and Steiner, Andreas and Caron, Mathilde and Geirhos, Robert and Alabdulmohsin, Ibrahim and others},
  journal={International Conference on Machine Learning},
  pages={7480--7512},
  year={2023},
  organization={PMLR},
  note={Large-scale vision transformer training and register token usage}
}

@article{wang2023internimage,
  title={InternImage: Exploring large-scale vision foundation models with deformable convolutions},
  author={Wang, Wenhai and Dai, Jifeng and Chen, Zhe and Huang, Zhenhang and Li, Zhiqi and Zhu, Xizhou and Hu, Xiaowei and Lu, Tong and Lu, Lewei and Li, Hongsheng and others},
  journal={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14408--14419},
  year={2023},
  note={Large-scale vision foundation model with advanced tokenization}
}

% Audio and Speech Processing

@article{baevski2020wav2vec,
  title={wav2vec 2.0: A framework for self-supervised learning of speech representations},
  author={Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={12449--12460},
  year={2020},
  note={Self-supervised speech representation learning with masked prediction}
}

@article{hsu2021hubert,
  title={HuBERT: Self-supervised speech representation learning by masked prediction of hidden units},
  author={Hsu, Wei-Ning and Bolte, Benjamin and Tsai, Yao-Hung Hubert and Lakhotia, Kushal and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={29},
  pages={3451--3460},
  year={2021},
  note={BERT-style pre-training for speech with discrete tokens}
}

% Code Generation and Programming Languages

@article{nijkamp2023codegen2,
  title={CodeGen2: Lessons for training LLMs on programming and natural languages},
  author={Nijkamp, Erik and Hayashi, Hiroaki and Xiong, Caiming and Savarese, Silvio and Zhou, Yingbo},
  journal={arXiv preprint arXiv:2305.02309},
  year={2023},
  note={Advanced code generation with programming language tokens}
}

@article{wang2021codet5,
  title={CodeT5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation},
  author={Wang, Yue and Wang, Weishi and Joty, Shafiq and Hoi, Steven CH},
  journal={arXiv preprint arXiv:2109.00859},
  year={2021},
  note={Code-specific pre-training with identifier tokens}
}

% Mathematical and Scientific Computing

@article{lample2019deep,
  title={Deep learning for symbolic mathematics},
  author={Lample, Guillaume and Charton, Fran{\c{c}}ois},
  journal={arXiv preprint arXiv:1912.01412},
  year={2019},
  note={Mathematical expression processing with symbolic tokens}
}

@article{hendrycks2021measuring,
  title={Measuring mathematical problem solving with the MATH dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2103.03874},
  year={2021},
  note={Mathematical reasoning benchmark and evaluation}
}

% Few-shot Learning and In-Context Learning

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020},
  note={GPT-3: In-context learning with demonstration formatting}
}

@article{min2022rethinking,
  title={Rethinking the role of demonstrations: What makes in-context learning work?},
  author={Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2202.12837},
  year={2022},
  note={Analysis of demonstration formatting and special token usage}
}

% Evaluation and Metrics

@article{papineni2002bleu,
  title={BLEU: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002},
  note={BLEU metric for sequence generation evaluation}
}

@article{lin2004rouge,
  title={ROUGE: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004},
  note={ROUGE metric for text summarization evaluation}
}

@article{zhang2019bertscore,
  title={BERTScore: Evaluating text generation with BERT},
  author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
  journal={arXiv preprint arXiv:1904.09675},
  year={2019},
  note={BERT-based evaluation metric using contextual embeddings}
}

% Computational Efficiency and Optimization

@article{fedus2022switch,
  title={Switch transformer: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={The Journal of Machine Learning Research},
  volume={23},
  number={1},
  pages={5232--5270},
  year={2022},
  note={Sparse transformer with routing tokens}
}

@article{kitaev2020reformer,
  title={Reformer: The efficient transformer},
  author={Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  journal={arXiv preprint arXiv:2001.04451},
  year={2020},
  note={Memory-efficient transformer with reversible layers}
}

% Robustness and Adversarial Analysis

@article{wallace2019universal,
  title={Universal adversarial triggers for attacking and analyzing NLP},
  author={Wallace, Eric and Feng, Shi and Kandpal, Nikhil and Gardner, Matt and Singh, Sameer},
  journal={arXiv preprint arXiv:1908.07125},
  year={2019},
  note={Adversarial attacks on transformer models via token manipulation}
}

@article{li2020bert,
  title={BERT-ATTACK: Adversarial attack against BERT using BERT},
  author={Li, Linyang and Ma, Ruotian and Guo, Qipeng and Xue, Xiangyang and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2004.09984},
  year={2020},
  note={Token-level adversarial attacks on BERT}
}

% Domain Adaptation

@article{kenton2019bert,
  title={BERT: Pre-training of deep bidirectional transformers for language understanding},
  author={Kenton, Jacob Devlin Ming-Wei Chang and Toutanova, Lee Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2019},
  note={Domain-specific fine-tuning strategies}
}

@article{gururangan2020dont,
  title={Don't stop pretraining: Adapt language models to domains and tasks},
  author={Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A},
  journal={arXiv preprint arXiv:2004.10964},
  year={2020},
  note={Domain-adaptive pre-training with special tokens}
}

% Recent Large Language Models

@article{chowdhery2022palm,
  title={PaLM: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022},
  note={Large-scale language model with advanced tokenization}
}

@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022},
  note={Chinchilla scaling laws and optimal training}
}

% Multimodal and Vision-Language Models

@inproceedings{lu2019vilbert,
  title={{ViLBERT}: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks},
  author={Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={13--23},
  year={2019},
  note={Multimodal transformer with cross-modal alignment tokens}
}

@inproceedings{tan2019lxmert,
  title={{LXMERT}: Learning Cross-Modality Encoder Representations from Transformers},
  author={Tan, Hao and Bansal, Mohit},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing},
  pages={5100--5111},
  year={2019},
  note={Vision-language model with specialized cross-modal tokens}
}

@article{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  journal={arXiv preprint arXiv:2103.00020},
  year={2021},
  note={CLIP model with image-text alignment tokens}
}

@article{li2022blip,
  title={{BLIP}: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},
  author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  journal={arXiv preprint arXiv:2201.12086},
  year={2022},
  note={Unified vision-language model with multimodal tokens}
}

@inproceedings{chen2020simple,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1597--1607},
  year={2020},
  note={SimCLR contrastive learning framework}
}

% Mixture of Experts and Scaling

@article{shazeer2017outrageously,
  title={Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal={arXiv preprint arXiv:1701.06538},
  year={2017},
  note={Sparse mixture of experts with routing tokens}
}

@article{fedus2022switch,
  title={Switch transformer: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={The Journal of Machine Learning Research},
  volume={23},
  number={1},
  pages={5232--5270},
  year={2022},
  note={Switch transformer with expert routing tokens}
}

@article{du2022glam,
  title={{GLaM}: Efficient Scaling of Language Models with Mixture-of-Experts},
  author={Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and others},
  journal={arXiv preprint arXiv:2112.06905},
  year={2021},
  note={GLaM mixture-of-experts with advanced routing}
}

% Efficient Transformers

@article{kitaev2020reformer,
  title={Reformer: The efficient transformer},
  author={Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  journal={arXiv preprint arXiv:2001.04451},
  year={2020},
  note={Memory-efficient transformer with LSH attention}
}

@article{zaheer2020big,
  title={Big bird: Transformers for longer sequences},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={17283--17297},
  year={2020},
  note={Long sequence transformer with global attention tokens}
}

@inproceedings{wang2020linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  booktitle={arXiv preprint arXiv:2006.04768},
  year={2020},
  note={Linear attention mechanism with projection tokens}
}

@article{child2019generating,
  title={Generating long sequences with sparse transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019},
  note={Sparse attention patterns for long sequences}
}
  year={2022},
  note={Chinchilla: Optimal compute scaling laws for language models}
}

% Additional Multimodal and Audio References for Chapter 5

@article{borsos2023audiolm,
  title={AudioLM: a language modeling approach to audio generation},
  author={Borsos, Zal{\'a}n and Marinier, Rapha{\"e}l and Vincent, Damien and Kharitonov, Eugene and Pietquin, Olivier and Sharifi, Matt and Roblek, Dominik and Teboul, Olivier and Grangier, David and Tagliasacchi, Marco and others},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={31},
  pages={2523--2533},
  year={2023},
  note={AudioLM: Language modeling for audio generation with discrete tokens}
}

@article{girdhar2023imagebind,
  title={ImageBind: One embedding space to bind them all},
  author={Girdhar, Rohit and El-Nouby, Alaaeldin and Liu, Zhuang and Singh, Mannat and Alwala, Kalyan Vasudev and Joulin, Armand and Misra, Ishan},
  journal={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={15180--15190},
  year={2023},
  note={ImageBind: Joint embedding space for six modalities}
}

@article{zellers2021merlot,
  title={MERLOT: Multimodal neural script knowledge models},
  author={Zellers, Rowan and Lu, Ximing and Hessel, Jack and Yu, Youngjae and Park, Jae Sung and Cao, Jize and Farhadi, Ali and Choi, Yejin},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={23634--23651},
  year={2021},
  note={MERLOT: Video-text understanding with temporal alignment tokens}
}

@article{huang2023audiogpt,
  title={AudioGPT: Understanding and generating speech, music, sound, and talking head},
  author={Huang, Rongjie and Ren, Yi and Liu, Jinglin and Cui, Chenye and Zhao, Zhou},
  journal={arXiv preprint arXiv:2304.12995},
  year={2023},
  note={AudioGPT: Multi-modal audio understanding with specialized tokens}
}

@article{driess2023palm,
  title={PaLM-E: An embodied multimodal language model},
  author={Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and others},
  journal={arXiv preprint arXiv:2303.03378},
  year={2023},
  note={PaLM-E: Embodied multimodal language model with robotic tokens}
}

@article{zhang2023video,
  title={Video-ChatGPT: Towards detailed video understanding via large vision and language models},
  author={Zhang, Hang and Li, Xin and Bing, Lidong},
  journal={arXiv preprint arXiv:2306.05424},
  year={2023},
  note={Video-ChatGPT: Video understanding with temporal tokens}
}

@article{liu2023visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={arXiv preprint arXiv:2304.08485},
  year={2023},
  note={LLaVA: Large Language and Vision Assistant with visual instruction tokens}
}

@article{li2023blip2,
  title={BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Silvio, Savarese and Hoi, Steven},
  journal={International Conference on Machine Learning},
  pages={19730--19742},
  year={2023},
  organization={PMLR},
  note={BLIP-2: Query tokens for bridging vision and language modalities}
}

@article{conneau2020unsupervised,
  title={Unsupervised cross-lingual representation learning for speech recognition},
  author={Conneau, Alexis and Baevski, Alexei and Collobert, Ronan and Mohamed, Abdelrahman and Auli, Michael},
  journal={arXiv preprint arXiv:2006.13979},
  year={2020},
  note={XLSR: Cross-lingual speech representations with language tokens}
}

@article{kong2020diffwave,
  title={DiffWave: A versatile diffusion model for audio synthesis},
  author={Kong, Zhifeng and Ping, Wei and Huang, Jiahao and Zhao, Kexin and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:2009.09761},
  year={2020},
  note={DiffWave: Diffusion-based audio synthesis with conditioning tokens}
}

@article{ramesh2022hierarchical,
  title={Hierarchical text-conditional image generation with CLIP latents},
  author={Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  journal={arXiv preprint arXiv:2204.06125},
  year={2022},
  note={DALL-E 2: Text-to-image generation with CLIP embedding tokens}
}

@article{saharia2022photorealistic,
  title={Photorealistic text-to-image diffusion models with deep language understanding},
  author={Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily L and Ghasemipour, Kamyar and Gontijo Lopes, Raphael and Karagol Ayan, Burcu and Salimans, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={36479--36494},
  year={2022},
  note={Imagen: Text-to-image diffusion with T5 text encoding tokens}
}

@article{mokady2022clipcap,
  title={ClipCap: CLIP prefix for image captioning},
  author={Mokady, Ron and Hertz, Amir and Bermano, Amit H},
  journal={arXiv preprint arXiv:2111.09734},
  year={2021},
  note={ClipCap: Image captioning with CLIP visual prefix tokens}
}

@article{wang2022ofa,
  title={OFA: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework},
  author={Wang, Peng and Yang, An and Men, Rui and Lin, Junyang and Bai, Shuai and Li, Zhikang and Ma, Jianxin and Zhou, Chang and Zhou, Jingren and Yang, Hongxia},
  journal={International Conference on Machine Learning},
  pages={23318--23340},
  year={2022},
  organization={PMLR},
  note={OFA: Unified multimodal framework with task-specific tokens}
}

@article{reed2022generalist,
  title={A generalist agent},
  author={Reed, Scott and Zolna, Konrad and Parisotto, Emilio and Colmenarejo, Sergio Gomez and Novikov, Alexander and Barth-Maron, Gabriel and Gimenez, Mai and Sulsky, Yury and Kay, Jackie and Springenberg, Jost Tobias and others},
  journal={Transactions on Machine Learning Research},
  year={2022},
  note={Gato: Generalist agent with multimodal tokenization across tasks}
}

% Additional Domain-Specific References for Chapter 6

@article{li2022competition,
  title={Competition-level code generation with alphacode},
  author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R{\'e}mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Dal Lago, Agustin and others},
  journal={Science},
  volume={378},
  number={6624},
  pages={1092--1097},
  year={2022},
  note={AlphaCode: Competitive programming with transformer models}
}

@article{wang2023codet5,
  title={CodeT5+: Open code large language models for code understanding and generation},
  author={Wang, Yue and Le, Hung and Gotmare, Akhilesh Deepak and Bui, Nghi DQ and Li, Junnan and Hoi, Steven CH},
  journal={arXiv preprint arXiv:2305.07922},
  year={2023},
  note={Multi-task code transformer with specialized tokens}
}

@article{roziere2023code,
  title={Code llama: Open foundation models for code},
  author={Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Remez, Tal and Rapin, J{\'e}r{\'e}my and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023},
  note={Code-specialized Llama models with programming language tokens}
}

@article{lewkowycz2022solving,
  title={Solving quantitative reasoning problems with language models},
  author={Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={3843--3857},
  year={2022},
  note={Minerva: Mathematical reasoning with specialized notation}
}

@article{trinh2024solving,
  title={Solving olympiad geometry without human demonstrations},
  author={Trinh, Trieu H and Wu, Yuhuai and Le, Quoc V and He, He and Luong, Thang},
  journal={Nature},
  volume={625},
  number={7995},
  pages={476--482},
  year={2024},
  note={AlphaGeometry: Symbolic reasoning in geometry}
}

@article{yang2023leandojo,
  title={LeanDojo: Theorem proving with retrieval-augmented language models},
  author={Yang, Kaiyu and Swope, Aidan and Gu, Alex and Chalamala, Rahul and Song, Peiyang and Yu, Shixing and Godil, Saad and Prenger, Ryan and Anandkumar, Anima},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={68426--68449},
  year={2023},
  note={Mathematical theorem proving with formal language tokens}
}

@article{azerbayev2023llemma,
  title={Llemma: An open language model for mathematics},
  author={Azerbayev, Zhangir and Schoelkopf, Hailey and Paster, Keiran and Dos Santos, Marco Tulio and McAleer, Stephen and Jiang, Albert Q and Deng, Jia and Biderman, Stella and Welleck, Sean},
  journal={arXiv preprint arXiv:2310.10631},
  year={2023},
  note={Open mathematical language model with LaTeX and formal notation}
}

@article{yu2018spider,
  title={Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql tasks},
  author={Yu, Tao and Zhang, Rui and Yang, Kai and Yasunaga, Michihiro and Wang, Dongxu and Li, Zifan and Ma, James and Li, Irene and Yao, Qingning and Roman, Shanelle and others},
  journal={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={3911--3921},
  year={2018},
  note={Large-scale text-to-SQL dataset with complex queries}
}

@article{scholak2021picard,
  title={Picard: Parsing incrementally for constrained auto-regressive decoding from language models},
  author={Scholak, Torsten and Schucher, Nathan and Bahdanau, Dzmitry},
  journal={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={9895--9901},
  year={2021},
  note={Constrained decoding for SQL generation}
}

@article{li2023can,
  title={Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls},
  author={Li, Jinyang and Hui, Binyuan and Qu, Ge and Yang, Jiaxi and Li, Binhua and Li, Bowen and Wang, Bailin and Qin, Bowen and Geng, Ruiying and Huo, Nan and others},
  journal={arXiv preprint arXiv:2305.03111},
  year={2023},
  note={Evaluation of language models for database interfaces}
}

@article{pourreza2024din,
  title={DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction},
  author={Pourreza, Mohammadreza and Rafiei, Davood},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={23426--23436},
  year={2023},
  note={Decomposed approach to text-to-SQL with error correction}
}

@article{gao2023text,
  title={Text-to-sql empowered by large language models: A benchmark evaluation},
  author={Gao, Dawei and Wang, Haibin and Li, Yaliang and Sun, Xiuyu and Qian, Yue and Ding, Bolin and Zhou, Jingren},
  journal={Proceedings of the VLDB Endowment},
  volume={17},
  number={1},
  pages={1--15},
  year={2023},
  note={Comprehensive evaluation of LLMs for SQL generation}
}

% Special Token Design and Architecture References for Chapter 7

@article{clark2019what,
  title={What does BERT look at? an analysis of BERT's attention},
  author={Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D},
  journal={Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={276--286},
  year={2019},
  note={Analysis of attention patterns and special token behavior in BERT}
}

@article{rogers2020primer,
  title={A primer on neural network models for natural language processing},
  author={Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
  journal={Journal of Artificial Intelligence Research},
  volume={61},
  pages={65--95},
  year={2020},
  note={Comprehensive survey of neural language models and embedding techniques}
}

@article{tenney2019bert,
  title={BERT rediscovers the classical NLP pipeline},
  author={Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
  journal={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={4593--4601},
  year={2019},
  note={Analysis of what linguistic information BERT embeddings capture}
}

@article{kenton2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Kenton, Jacob Devlin Ming-Wei Chang and Toutanova, Kristina},
  journal={Proceedings of NAACL-HLT},
  pages={4171--4186},
  year={2019},
  note={Original BERT paper with detailed special token design}
}

@article{liu2019roberta,
  title={RoBERTa: A robustly optimized BERT pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019},
  note={Optimization techniques for transformer training and token handling}
}

@article{wang2019glue,
  title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={353--355},
  year={2019},
  note={Standard benchmark for evaluating language understanding models}
}

@article{strubell2019energy,
  title={Energy and policy considerations for deep learning in NLP},
  author={Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  journal={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={3645--3650},
  year={2019},
  note={Analysis of computational efficiency in transformer models}
}

@article{michel2019sixteen,
  title={Are sixteen heads really better than one?},
  author={Michel, Paul and Levy, Omer and Neubig, Graham},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={14014--14024},
  year={2019},
  note={Analysis of attention head importance and pruning strategies}
}

@article{voita2019analyzing,
  title={Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned},
  author={Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
  journal={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={5797--5808},
  year={2019},
  note={Understanding of attention specialization and token processing}
}

@article{hewitt2019structural,
  title={A structural probe for finding syntax in word representations},
  author={Hewitt, John and Manning, Christopher D},
  journal={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={4129--4138},
  year={2019},
  note={Probing techniques for analyzing learned token representations}
}

@article{tenney2019what,
  title={What do you learn from context? probing for sentence structure in contextualized word representations},
  author={Tenney, Ian and Xia, Patrick and Chen, Berlin and Wang, Alex and Poliak, Adam and McCoy, R Thomas and Kim, Najoung and Van Durme, Benjamin and Bowman, Samuel R and Das, Dipanjan and others},
  journal={International Conference on Learning Representations},
  year={2019},
  note={Comprehensive probing study of contextualized representations}
}

@article{reif2019visualizing,
  title={Visualizing and measuring the geometry of BERT},
  author={Reif, Emily and Yuan, Ann and Wattenberg, Martin and Viegas, Fernanda B and Coenen, Andy and Pearce, Adam and Kim, Been},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={8594--8603},
  year={2019},
  note={Analysis of embedding space geometry and token relationships}
}

@article{ethayarajh2019contextual,
  title={How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings},
  author={Ethayarajh, Kawin},
  journal={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing},
  pages={55--65},
  year={2019},
  note={Comparative analysis of contextualized embedding spaces}
}

@article{jawahar2019does,
  title={What does BERT learn about the structure of language?},
  author={Jawahar, Ganesh and Sagot, Beno{\^i}t and Seddah, Djam{\'e}},
  journal={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={3651--3657},
  year={2019},
  note={Layer-wise analysis of linguistic knowledge in BERT}
}

% Chapter 8 and 9 References: Optimization, Training, and Advanced Techniques

@article{child2019generating,
  title={Generating long sequences with sparse transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019},
  note={Sparse attention patterns for computational efficiency}
}

@article{zaheer2020big,
  title={Big bird: Transformers for longer sequences},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Vaswani, Anirudh and Wang, Qifan and Yang, Li and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={17283--17297},
  year={2020},
  note={Sparse attention mechanisms for long sequences}
}

@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020},
  note={Attention patterns for long documents with local-global attention}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022},
  note={Chain-of-thought reasoning with step-by-step prompting}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022},
  note={Zero-shot reasoning capabilities with Let's think step by step}
}

@article{zhou2022least,
  title={Least-to-most prompting enables complex reasoning in large language models},
  author={Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Cui, Claire and Bousquet, Olivier and Le, Quoc and others},
  journal={arXiv preprint arXiv:2205.10625},
  year={2022},
  note={Decomposed reasoning approach for complex problems}
}

@article{yao2024tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={11809--11822},
  year={2023},
  note={Tree-structured reasoning and deliberate problem solving}
}

@article{schick2023toolformer,
  title={Toolformer: Language models can teach themselves to use tools},
  author={Schick, Timo and Dwivedi-Yu, Jane and Dess{\`i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={68539--68551},
  year={2023},
  note={Self-supervised learning for tool use and API interaction}
}

@article{qin2023tool,
  title={Tool learning with foundation models},
  author={Qin, Yujia and Liang, Shihao and Ye, Yining and Zhu, Kunlun and Yan, Lan and Lu, Yaxi and Lin, Yankai and Cong, Xin and Tang, Xiangru and Qian, Bill and others},
  journal={arXiv preprint arXiv:2304.08354},
  year={2023},
  note={Comprehensive framework for tool learning with language models}
}

@article{patil2023gorilla,
  title={Gorilla: Large language model connected with massive apis},
  author={Patil, Shishir G and Zhang, Tianjun and Wang, Xin and Gonzalez, Joseph E},
  journal={arXiv preprint arXiv:2305.15334},
  year={2023},
  note={API-aware language model for tool use and function calling}
}

@article{rae2021scaling,
  title={Scaling language models: Methods, analysis \& insights from training gopher},
  author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal={arXiv preprint arXiv:2112.11446},
  year={2021},
  note={Large-scale training methodologies and optimization techniques}
}

@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022},
  note={Chinchilla scaling laws and compute-optimal training}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023},
  note={Efficient training and inference for foundation models}
}

@article{dao2022flashattention,
  title={FlashAttention: Fast and memory-efficient exact attention with IO-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16344--16359},
  year={2022},
  note={Memory-efficient attention computation}
}

@article{fedus2022switch,
  title={Switch transformer: Scaling to trillions of parameters with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={120},
  pages={1--39},
  year={2022},
  note={Sparse mixture of experts for efficient scaling}
}

@article{shazeer2017outrageously,
  title={Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal={arXiv preprint arXiv:1701.06538},
  year={2017},
  note={Mixture of experts architecture for conditional computation}
}

@article{kitaev2020reformer,
  title={Reformer: The efficient transformer},
  author={Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  journal={arXiv preprint arXiv:2001.04451},
  year={2020},
  note={Memory-efficient transformer with reversible layers}
}

@article{tay2022efficient,
  title={Efficient transformers: A survey},
  author={Tay, Yi and Dehghani, Mostafa and Rao, Jinfeng and Fedus, William and Abnar, Samira and Chung, Hyung Won and Narang, Sharan and Yogatama, Dani and Vaswani, Anirudh and Metzler, Donald},
  journal={ACM Computing Surveys},
  volume={55},
  number={6},
  pages={1--28},
  year={2022},
  note={Comprehensive survey of efficient transformer architectures}
}